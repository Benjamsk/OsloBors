{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Chapter 1: Scraping OsloBors stock data\n",
    "\n",
    "In this chapter we get our first data set from the OsloBors, Scandinavia's only independent stock exchange. \n",
    "\n",
    "#####The Imports\n",
    "Most of the hard work in this chapter and the following chapters is done using Pandas, Pythons \"library providing high-performance, easy-to-use data structures and data analysis tools\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import StringIO\n",
    "import pandas as pd\n",
    "from pandas import read_html\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pandas.io.data import DataReader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For visulisations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#From python 3, dont have to worry about divide with floating numbers\n",
    "from __future__ import division "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####MongoDB: Where to save the data?\n",
    "When we scrape stock information we want we do not want to have to rescrape it everytime we do some analysis. There are a number of methods to save the data, JSON, CSV, MYSQL, Python Pickle file, but I have decided to use MongoDB. \n",
    "\n",
    "MongoDB is a NoSQL database that stores information as Key:document pairs. Unlike a CSV file or equivalent, you can run powerful queries agasint MongoDB and make updates without loading and re-saving the entire file everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Setup MongoDB\n",
    "#Switch on my file server then run in bash: mongod --dbpath /Volumes/Data/David_Files/data/db/\n",
    "client = MongoClient()\n",
    "db = client.oslo_bors #This is our database name\n",
    "collectionStocks = db.stocks #stocks is our table (Collection) within the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Creating a list of tickers\n",
    "The first thing we need to know is a list of stock tickers, which we want to scrape data from. The OsloBors provides a list of all active stocks and their tickers in the form of a html table which we can get by following a given url. However, this html table is loaded via javascript, this means we cannot simply extract the html table using an html parser. The reason is because most html parsers, like beautiful soup, typically do not render javascript code. So we us a library selenium to open the url in the firefox browser, which does render javascript code. Then we extract the html from the broswer and use pandas to get extract our table and therefore our ticket list. \n",
    "\n",
    "The end result should look something like this.\n",
    "\n",
    "tickers = ['ASC', 'AFG',...,'ZONC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load the webpage in firefox and extract the content\n",
    "from selenium import webdriver\n",
    "browser = webdriver.Firefox()\n",
    "browser.get('http://www.oslobors.no/markedsaktivitet/#/list/shares/quotelist/ob/all/false')\n",
    "content = browser.page_source\n",
    "browser.quit()\n",
    "\n",
    "#Save the list of tickers\n",
    "stocks = pd.read_html(content)[0]\n",
    "tickers = stocks['Ticker'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####The link to stock data\n",
    "The following links leads to statoils (STL) excel sheet containing 5 years worth of stock data. We use this link, replacing STL with each ticker in our ticker list to extract data for all our stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The link for the statoil stock excel 5 years\n",
    "link = 'http://www.oslobors.no/ob/servlets/excel?type=history&columns=DATE%2C+CLOSE_CA%2C+BID_CA%2C+ASK_CA%2C+HIGH_CA%2C+LOW_CA%2C+TURNOVER_TOTAL%2C+VOLUME_TOTAL_CA%2C+TRADES_COUNT%2C+TRADES_COUNT_TOTAL%2C+VWAP&format[DATE]=ddd.mm.YY&format[CLOSE_CA]=%23%2C%23%230.00%23%23%23&format[BID_CA]=%23%2C%23%230.00%23%23&format[ASK_CA]=%23%2C%23%230.00%23%23&format[HIGH_CA]=%23%2C%23%230.00%23%23%23&format[LOW_CA]=%23%2C%23%230.00%23%23%23&format[TURNOVER_TOTAL]=%23%2C%23%230&format[VOLUME_TOTAL_CA]=%23%2C%23%230&format[TRADES_COUNT]=%23%2C%23%230&format[TRADES_COUNT_TOTAL]=%23%2C%23%230&format[VWAP]=%23%2C%23%230.00%23%23%23&header[DATE]=STL&header[CLOSE_CA]=Last&header[BID_CA]=Bid&header[ASK_CA]=Ask&header[HIGH_CA]=High&header[LOW_CA]=Low&header[TURNOVER_TOTAL]=Total%20turnover%20%28NOK%29&header[VOLUME_TOTAL_CA]=Total%20amount%20shares%20volume&header[TRADES_COUNT]=Amount%20off.%20trades&header[TRADES_COUNT_TOTAL]=Amount%20trades%20total&header[VWAP]=VWAP&view=DELAYED&source=feed.ose.quotes.INSTRUMENTS&filter=ITEM_SECTOR%3D%3DsSTL.OSE%26%26DELETED!%3Dn1&stop=now&start=1286402400000&space=DAY&ascending=true&limit=10000000&filename=data.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Downloading the stock data\n",
    "The following code simply runs a loop over all the tickers in our ticker list, puts them in the statoil URL link and downloads the excel file as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASC - Scraping\n",
      "ASC - Added to the database\n",
      "...",
      "ZONC - Scraping\n",
      "ZONC - Not scraped, see comment\n"
     ]
    }
   ],
   "source": [
    "collectionStocks.remove() #Remove the old data from the MongoDB database\n",
    "\n",
    "for i in tickers:\n",
    "    \n",
    "    print str(i) + \" - Scraping\"\n",
    "    \n",
    "    #Extract the Data from the excel\n",
    "    ticker = str(i).replace(\" \",\"%20\") #Some tickers have spaces in their names ('SAS NOK'), so we replace the space with a %20 to not break the URL\n",
    "    url = link.replace(\"STL\", ticker) #Swap out statoils ticker for the one in our list\n",
    "    xld = urllib2.urlopen(url).read()\n",
    "    xlds = StringIO.StringIO(xld)\n",
    "    data = pd.read_excel(xlds, \"data\") #Turn the excel file into a pandas dataframe\n",
    "    \n",
    "    data.columns = data.columns.str.replace('.','') #The column names are keys in mongoDB and mongoDB doesnt like '.' in its key names\n",
    "    \n",
    "    data.rename(columns = {str(i):'Date'}, inplace=True) #Name the date column to date, it is orginally called after the ticker\n",
    "    \n",
    "    data['Ticker'] = str(i) #Add a ticker column\n",
    "    \n",
    "    records = json.loads(data.T.to_json(date_format='iso')).values() #Convert dataframe to json, which is a format that can be directly loaded into mongoDB\n",
    "    \n",
    "    #Here we check for any errors before saving the data to mongoDB\n",
    "    if len(records) > 0:\n",
    "        collectionStocks.insert(records) #Store the data in the mongoDB\n",
    "        \n",
    "        print str(i) + \" - Added to the database\" #Current stock we are scraping\n",
    "    else:\n",
    "        print str(i) + \" - Not scraped, see comment\" #some tickers use OAX and not OSE in its link string, so our statoil link returns empty for these tickers. For now we just ignore them\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Test if our data saved correctly\n",
    "Just to make sure mongoDB was working as I thought it should, I extract a signle row from the database and load it into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = collectionStocks.find_one() #Find a single value in our mongoDB database and check it isn't empty\n",
    "test = pd.DataFrame.from_dict([test_data]) #Load that data into a pandas dataframe\n",
    "#print test.columns.values[2].encode(encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Finished\n",
    "We now have 5 years of OsloBors data for most of our ticker list saved into our database. The next step is to get a basic understanding of our data by doing some stock analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
